Large language models like Chat GPT can answer almost all types of questions but when it comes to providing authoritative answers with sources, these models get a bit stuck because they give answers on the basis of their training data. They do not take that answer from real time sources and their training data is not always up to date. So what is the solution to this problem? Here comes an excellent technique called retrieve.Augmented Generation or RAG Let us understand what this RAG is, how it works and what can be done using it. Hello, I am Vikram Singh Chandel Your own Analytics [Music] [Praise] [Music] Guru Retrieve Augmented Generation or RAG is a technique that makes generative AI models more accurate and reliable by bringing facts from external sources. Basically this technologyFixes the issue of accuracy and reliability of LLM. With this, LLMs are able to answer general prompts faster. When something has to be generated on a current or more specific topic, then this technique becomes even more useful. There were a total of 12 people who developed Ruck, the main of which was Patrick Lewis who coined the name Ragna. Lewis and his 11 colleagues together created it so that generativeAI services can be linked to external resources, especially those with the latest technical details. Louis wrote a paper with his colleagues in 2020. In it, Rack was called a general purpose fine-tuning recipe because almost any LLM can be connected to practically any external resource.So that users can check any claim themselves, this builds trust on the models. A very good and useful example of this is Perplexity AI which is an answer machine type model. It is a combination of RUGS and LLM where users can ask anything and the model answers them with sources. Then they can also cross check the given answer. Due to this technique, the chances of LLM guessing anything wrong are reduced becauseThe models already have a source that provides the proper context, so they do much less learning. Another big advantage of Ruck is that it is relatively easy. Louis and three other co-authors of the original paper explained in a blog that developers can implement this process in just five lines of code, making this method faster and less expensive than fine-tuning and allowing users to learn on the go.Sources can be hot swapped. Hot swap means changing models or sources without turning off the system. But how does this rack work? First it takes the data from a database document website or API and then divides them into small chunks and converts them into vector representations or embeddings. Then these vector embeddings are stored in a specialized vector database so that when the time comes according to the user's query.Here, when a user gives a query, it is also converted into a vector embedding, then that query embedding is compared with the vector database and the top relevant chunks are retrieved so that these retrieved chunks can be merged with the original query to form an augmented context. After this, this augmented context is fed into an LLM model such as GPT.4 or Clot 3 is then fed to the final output of the generated response model which provides relevant and information rich answers to the user's queries. Ruck can be used in conjunction with LLM or independently. When used with LLM, responses are more fluent. Context rich because it provides LLM with relevant and up to date information from external sources. With Ruck youPractically, it can talk to any data collection, which opens up many new possibilities. For example, a generative AI model implemented with a medical index can be a very helpful assistant for a doctor or nurse in a hospital. The same thing can also be made for a financial analyst which is linked to market data, which means that any business can convert its technical or policy manual videos or logs into a knowledge base.
I hope you have understood well about the rack from this video, but if you have any question in your mind, then do ask in the comments and do tell me what you think about this technique and what other applications it can have. Let's meet now in the next video. Till then stay cool and keep learning and teaching.