# Educational Notes

## Summary
 Adaptive gradient method is an extension of the gradient descent optimization and which is allowing different learning rate for each and every weight when it is applying the back propagation method in the real world data set a few input features are sparse sparse in the sense most of the features will have values zero in the input data set . In this kind of scenario the same learning rate will not giv giv the same step size for the learning rate whenever we are updating the parameter for the given input features you look at that here in this cost function or gradient descent is using uh the same Alpha value for all the epochs are iterations okay so that is creating the issue here which is need of different amount of curvatures in the different dimensions that means we look at here

## Key Points
- so we are going to calculate the gradient of the pr evious uh using the previous weight for the calculating of new weight parameter so here ETA T is having a different learning rate you look at that Alpha T is the different learning rate for each weight at each iteration so the which is giving in the denominator ETA T divided by square root of alpha t cap plus uh Epsilon so here you have Epsilon is giving you small positive constant to avoid the error which is caused by division by zero that means when our Alpha T becomes 0 then this term will become 0 okay so when you are not having the Epsilon then ETA by anything ETA by 0 will give you infinity so um uh it will give you small updation okay so it will not give you uh updation uh uh updation to the uh weight parameter when it is becoming Infinity okay so that s why uh so when it is becoming 0 this also will be coming zero so that s why uh it will not give you uh the faster convergence So to avoid that we are going to use Epsilon value here okay so Epsilon value gives you small positive constant then uh the E30 when it is becoming 0 also it will give you some positive constant it will make the uh small updation in the learning rate E30 so that s why we are using the Epsilon here the alpha T is having the squared gradient value with respect to the previous weight step okay so when it is using the squaring gradients which is giving you updation uh in the alpha t uh which is always greater than the alpha for T minus 1 and also when it is squaring up the negative gradient suppose if it is available like that which is also bringing the positive value right like this so always the alpha T value will become positive constraint when uh you are looking this equation Alpha t Okay Alpha T is inversionally proportional inversely proposed to the E30 okay so from this equation when you are increasing the alpha T automatically ETA T is becoming low so this makes the alpha T Alpha uh Alpha T makes the equation uh adaptively update the E30 value that means this uh Alpha T is changes the uh E30 adaptively for each and every iterations so when uh the is decreasing it needs more number of iterations to reach the convergence point but the uh the alpha T is updating the 830 by changing this Alpha T at every iteration so we need not to select the learning rate manually like another algorithms so and also Alpha uh T makes the um for a mix the learning rate uh learning rate changes at each Epoch here like this so this is making the converging uh faster than other algorithms so advantages of this other grid is here no manual tuning is needed for alpha alpha and uh due to giving different Alpha rates for uh different parameters for each iteration making the faster convergence and more reliable one of the disadvantages here when Alpha T is becoming large suppose uh the ETA E30 is inversionally proportional to Alpha T okay so when Alpha T becomes larger value for example we are giving 5 or 50 like that so when it is increasing like this the ETA T will become less okay so it needs more number of iterations when you are getting very minimum value here you see E30 is very less than you will get many minimum updation minimum change in the new weight value so it makes overseer convergence uh to reach the uh value the cost close to the zero so this is one of the main advantage in the other grade thanks for watching
- 3 we need some uh some higher value adjustment okay so some higher value adjustment so this kind of uh uh updation can be done by using the different learning rates okay so learning rate should be different to make this kind of adjustment to uh make the converging process faster so that is the um purpose the order grid is using a different learning rate for each and every parameters during the training phase the weight update rule is here given or the the new weight is updated with the previous step weight WT minus 1 minus ETA T so that means this is the learning rate we are using here and the gradient of uh error with respect to uh W of T minus 1
- hi everyone the other grade optimization is called as adaptive gradient here which is extension of the gradient descent algorithm uh which is using negative gradient of an objective function to locate the log Global minimum of the function so the global minimum will give you the minimum error for your cost function the limitation of the gradient descent is here which is using same step size for the learning rate whenever we are updating the parameter for the given input features you look at that here in this cost function or the gradient descent is using uh the same Alpha value for all the epochs are iterations okay so that is creating the issue here which is need of different amount of curvatures in the different dimensions that means you look at here this is the stochastic gradient learning curve which is taking many number of oscillations and also different dimension different size of isolations with magnitude to reach the convergence point so we need uh different uh learning rate for learning rate for each and every parameter given with the input features to make that uh learning curve smoother under reach the converging Point faster so for that kind of issue can be solved by using the Adaptive gradient method which is an extension of the gradient descent optimization and which is allowing different learning rate for each and every weight when it is applying the back propagation method in the real world data set a few input features are sparse sparse in the sense most of the features will have values zero in the input data set suppose when you are having the input data set here we will have the input features uh X1 X2 etc etc x n at that time most of the features will have the value zeros okay so that type of uh features are called sparse input feature suppose if we have uh values with the non zero values like one two zero point one like that then that kind of features are called dense features okay so in this kind of scenario the same learning rate will not give you the good optimization that means it will not reach the convergence Point faster so we need a different learning rate for each and every Epoch or iteration by adapting the learning rate to each and every parameter available in the input features so the other grid is using low Landing rates for the parameters which are associated with the frequently occurring features but which is using High Learning rates for parameters which are associated with infrequently occurred features are rare features so when we are using different learning rate for different kind of features will suitable for the real time data set which is having sparse data for example uh during uh the training phase some features might reached near to the converging stage where only it needs a small adjustment that means suppose um our two weights weight one is a 0
- 3 okay so you assume this is the zero value of value for converging Point okay so we have to bring this two parameters in uh towards the 0
- 3 we need small adjustment Okay small adjustment that means small updation right but when you are um bringing this 0
- 9 right you assume that we need to bring this weights into uh the convergence point for example 0
- 3 so when we are are bringing this W1 to the 0
- 5 and weight 2 is here we are having 0

## Core Concepts
- Main ideas and important information
- Essential understanding from the content

## Applications
- Practical usage and examples
- Real-world implementation
