e order grid is using a different learning rate for each and every parameters during the training phase the weight update rule is here given or the the new weight is updated with the previous step weight WT minus 1 minus ETA T so that means this is the learning rate we are using here and the gradient of uh error with respect to uh W of T minus 1. so we are going to calculate the gradient of the pr evious uh using the previous weight for the calculating of new weight parameter so here ETA T is having a different learning rate you look at that Alpha T is the different learning rate for each weight at each iteration so the which is giving in the denominator ETA T divided by square root of alpha t cap plus uh Epsilon so here you have Epsilon is giving you small positive constant to avoid the error which is caused by division by zero that means when our Alpha T becomes 0 then this term will become 0 okay so when you are not having the Epsilon then ETA by anything ETA by 0 will give you infi