ecome 0 okay so when you are not having the Epsilon then ETA by anything ETA by 0 will give you infinity so um uh it will give you small updation okay so it will not give you uh updation uh uh updation to the uh weight parameter when it is becoming Infinity okay so that s why uh so when it is becoming 0 this also will be coming zero so that s why uh it will not give you uh the faster convergence So to avoid that we are going to use Epsilon value here okay so Epsilon value gives you small positive constant then uh the E30 when it is becoming 0 also it will give you some positive constant it will make the uh small updation in the learning rate E30 so that s why we are using the Epsilon here the alpha T is having the squared gradient value with respect to the previous weight step okay so when it is using the squaring gradients which is giving you updation uh in the alpha t uh which is always greater than the alpha for T minus 1 and also when it is squaring up the negative gradient suppose