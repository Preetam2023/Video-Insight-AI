hi everyone the other grade optimization is called as adaptive gradient here which is extension of the gradient descent algorithm uh which is using negative gradient of an objective function to locate the log Global minimum of the function so the global minimum will give you the minimum error for your cost function the limitation of the gradient descent is here which is using same step size for the learning rate whenever we are updating the parameter for the given input features you look at that here in this cost function or the gradient descent is using uh the same Alpha value for all the epochs are iterations okay so that is creating the issue here which is need of different amount of curvatures in the different dimensions that means you look at here this is the stochastic gradient learning curve which is taking many number of oscillations and also different dimension different size of isolations with magnitude to reach the convergence point so we need uh different uh learning rate f