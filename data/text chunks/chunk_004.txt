and then divides them into small chunks and converts them into vector representations or embeddings. Then these vector embeddings are stored in a specialized vector database so that when the time comes according to the user s query.Here, when a user gives a query, it is also converted into a vector embedding, then that query embedding is compared with the vector database and the top relevant chunks are retrieved so that these retrieved chunks can be merged with the original query to form an augmented context. After this, this augmented context is fed into an LLM model such as GPT.4 or Clot 3 is then fed to the final output of the generated response model which provides relevant and information rich answers to the user s queries. Ruck can be used in conjunction with LLM or independently. When used with LLM, responses are more fluent. Context rich because it provides LLM with relevant and up to date information from external sources.