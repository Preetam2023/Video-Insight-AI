isolations with magnitude to reach the convergence point so we need uh different uh learning rate for learning rate for each and every parameter given with the input features to make that uh learning curve smoother under reach the converging Point faster so for that kind of issue can be solved by using the Adaptive gradient method which is an extension of the gradient descent optimization and which is allowing different learning rate for each and every weight when it is applying the back propagation method in the real world data set a few input features are sparse sparse in the sense most of the features will have values zero in the input data set suppose when you are having the input data set here we will have the input features uh X1 X2 etc etc x n at that time most of the features will have the value zeros okay so that type of uh features are called sparse input feature suppose if we have uh values with the non zero values like one two zero point one like that then that kind of fea